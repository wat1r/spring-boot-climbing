# 大数据笔记-有道云

## 0.大数据实战项目

### 0.1.大数据实战项目

#### 0.1.1.大数据项目之推荐系统<电影推荐系统>

- 推荐算法有哪些？

  - 基于内容的推荐算法

  - 基于协同过滤的推荐

  - 基于关联规则的推荐

  - 基于模型的推荐

- 组合推荐

![在这里插入图片描述](https://img-blog.csdnimg.cn/474af66fca9b40d0b8d0124a303db9b6.png)

![image-20220617223212005](/Users/frankcooper/Library/Application Support/typora-user-images/image-20220617223212005.png)

#### 0.1.2.大数据项目之新闻项目 zip包

  





## 1.MapReduce

###  1.1.MapReduce程序运行流程分析

![在这里插入图片描述](https://img-blog.csdnimg.cn/42245819655141ec8cc8516e353f6b48.png)

1）在MapReduce程序读取文件的输入目录上存放相应的文件。

2）客户端程序在submit()方法执行前，获取待处理的数据信息，然后根据集群中参数的配置形成一个任务分配规划。

3）客户端提交job.split、jar包、job.xml等文件给yarn，yarn中的resourcemanager启动MRAppMaster。

4）MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程。

5）maptask利用客户指定的inputformat来读取数据，形成输入KV对。

6）maptask将输入KV对传递给客户定义的map()方法，做逻辑运算

7）map()运算完毕后将KV对收集到maptask缓存。

8）maptask缓存中的KV对按照K分区排序后不断写到磁盘文件

9）MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据分区。

10）Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算。

11）Reducetask运算完毕后，调用客户指定的outputformat将结果数据输出到外部存储。

MapReduce的工作流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/020738f4daa541dc8dcc70ae3368fa3d.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/bb30492bcba447d5a41dc49d9c95d558.png)

### 1.2.流程详解

上面的流程是整个mapreduce最全工作流程，但是shuffle过程只是从第7步开始到第16步结束，具体shuffle过程详解，如下：

1）maptask收集我们的map()方法输出的kv对，放到内存缓冲区中

2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件

3）多个溢出文件会被合并成大的溢出文件

4）在溢出过程中，及合并的过程中，都要调用partitoner进行分组和针对key进行排序

5）reducetask根据自己的分区号，去各个maptask机器上取相应的结果分区数据

6）reducetask会取到同一个分区的来自不同maptask的结果文件，reducetask会将这些文件再进行合并（归并排序）

7）合并成大文件后，shuffle的过程也就结束了，后面进入reducetask的逻辑运算过程（从文件中取出一个一个的键值对group，调用用户自定义的reduce()方法）

### 1.3.注意

Shuffle中的缓冲区大小会影响到mapreduce程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度就越快。

缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认100M

#### Shuffle机制

Mapreduce确保每个reducer的输入都是按键排序的。系统执行排序的过程（即将map输出作为输入传给reducer）称为shuffle。

![在这里插入图片描述](https://img-blog.csdnimg.cn/9e9f85d74c46434192c60a8854fb1ed5.png)

## 2.HDFS

### 2.1.HDFS读写数据的过程

![在这里插入图片描述](https://img-blog.csdnimg.cn/9cc6524027d64742804ae5bcde36c757.png)

1）客户端向namenode请求下载文件，namenode通过查询元数据，找到文件块所在的datanode地址。

2）挑选一台datanode（就近原则，然后随机）服务器，请求读取数据。

3）datanode开始传输数据给客户端（从磁盘里面读取数据放入流，以packet为单位来做校验）。

4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。

![在这里插入图片描述](https://img-blog.csdnimg.cn/58c58cec141f40f49403f991244d18d7.png)

1）客户端向namenode请求上传文件，namenode检查目标文件是否已存在，父目录是否存在。

2）namenode返回是否可以上传。

3）客户端请求第一个 block上传到哪几个datanode服务器上。

4）namenode返回3个datanode节点，分别为dn1、dn2、dn3。

5）客户端请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成

6）dn1、dn2、dn3逐级应答客户端

7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答

8）当一个block传输完成之后，客户端再次请求namenode上传第二个block的服务器。（重复执行3-7步）

[hadoop之HDFS启动过程详解](https://blog.csdn.net/qq_32641659/article/details/87930118)

###  2.2.网络拓扑概念

​    在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。

​    节点距离：两个节点到达最近的共同祖先的距离总和。

例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述。

Distance(/d1/r1/n1, /d1/r1/n1)=0（同一节点上的进程）

Distance(/d1/r1/n1, /d1/r1/n2)=2（同一机架上的不同节点）

Distance(/d1/r1/n1, /d1/r3/n2)=4（同一数据中心不同机架上的节点）

Distance(/d1/r1/n1, /d2/r4/n2)=6（不同数据中心的节点）

![在这里插入图片描述](https://img-blog.csdnimg.cn/d6646f9bf2734a42bae7cb93823162ba.png)

大家算一算每两个节点之间的距离。

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/b1315a8c2c4f42098fdb4a4a7b24959d.png)

### 2.3.机架感知（副本节点选择）

#### 1）官方ip地址：

http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-common/RackAwareness.html

http://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication

#### 2）低版本Hadoop副本节点选择

第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。

第二个副本和第一个副本位于不相同机架的随机节点上。

第三个副本和第二个副本位于相同机架，节点随机。

![在这里插入图片描述](https://img-blog.csdnimg.cn/9075f9de0b414383a50c52df6f2e73e7.png)

#### 3）Hadoop2.7.2副本节点选择

​    第一个副本在client所处的节点上。如果客户端在集群外，随机选一个。

​    第二个副本和第一个副本位于相同机架，随机节点。

​    第三个副本位于不同机架，随机节点。

![在这里插入图片描述](https://img-blog.csdnimg.cn/7742c0e690954d31b2bc635ec4e94d9a.png)





### 2.4.NameNode工作机制

#### NameNode&Secondary NameNode工作机制

![在这里插入图片描述](https://img-blog.csdnimg.cn/93d092108bd14140b0cf55b4c6c0b9d1.png)

##### 1）第一阶段：namenode启动

（1）第一次启动namenode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。

（2）客户端对元数据进行增删改的请求

（3）namenode记录操作日志，更新滚动日志。

（4）namenode在内存中对数据进行增删改查

##### 2）第二阶段：Secondary NameNode工作

​    （1）Secondary NameNode询问namenode是否需要checkpoint。直接带回namenode是否检查结果。

​    （2）Secondary NameNode请求执行checkpoint。

​    （3）namenode滚动正在写的edits日志

​    （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode

​    （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。

​    （6）生成新的镜像文件fsimage.chkpoint

​    （7）拷贝fsimage.chkpoint到namenode

​    （8）namenode将fsimage.chkpoint重新命名成fsimage



### 2.5.DataNode工作机制

#### DataNode工作机制

![在这里插入图片描述](https://img-blog.csdnimg.cn/f804358122804a76a28278c5f12fefac.png)

1）一个数据块在datanode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。

2）DataNode启动后向namenode注册，通过后，周期性（1小时）的向namenode上报所有的块信息。

3）心跳是每3秒一次，心跳返回结果带有namenode给该datanode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个datanode的心跳，则认为该节点不可用。

4）集群运行中可以安全加入和退出一些机器

**数据完整性**

1）当DataNode读取block的时候，它会计算checksum

2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。

3）client读取其他DataNode上的block.

4）datanode在其文件创建后周期验证checksum

### 2.6.HDFS HA高可用

#### 2.6.1.HA概述

1）所谓HA（high available），即高可用（7*24小时不中断服务）。

2）实现高可用最关键的策略是消除单点故障。HA严格来说应该分成各个组件的HA机制：HDFS的HA和YARN的HA。

3）Hadoop2.0之前，在HDFS集群中NameNode存在单点故障（SPOF）。

4）NameNode主要在以下两个方面影响HDFS集群

​    NameNode机器发生意外，如宕机，集群将无法使用，直到管理员重启

​    NameNode机器需要升级，包括软件、硬件升级，此时集群也将无法使用

HDFS HA功能通过配置Active/Standby两个nameNodes实现在集群中对NameNode的热备来解决上述问题。如果出现故障，如机器崩溃或机器需要升级维护，这时可通过此种方式将NameNode很快的切换到另外一台机器。

#### 2.6.2.HDFS-HA工作机制

1）通过双namenode消除单点故障

##### 1 HDFS-HA工作要点

1）元数据管理方式需要改变：

内存中各自保存一份元数据；

Edits日志只有Active状态的namenode节点可以做写操作；

两个namenode都可以读取edits；

共享的edits放在一个共享存储中管理（qjournal和NFS两个主流实现）；

2）需要一个状态管理功能模块

实现了一个zkfailover，常驻在每一个namenode所在的节点，每一个zkfailover负责监控自己所在namenode节点，利用zk进行状态标识，当需要进行状态切换时，由zkfailover来负责切换，切换时需要防止brain split现象的发生。

3）必须保证两个NameNode之间能够ssh无密码登录。

4）隔离（Fence），即同一时刻仅仅有一个NameNode对外提供服务

#####  HDFS-HA自动故障转移工作机制

前面学习了使用命令hdfs haadmin -failover手动进行故障转移，在该模式下，即使现役NameNode已经失效，系统也不会自动从现役NameNode转移到待机NameNode，下面学习如何配置部署HA自动进行故障转移。自动故障转移为HDFS部署增加了两个新组件：ZooKeeper和ZKFailoverController（ZKFC）进程。ZooKeeper是维护少量协调数据，通知客户端这些数据的改变和监视客户端故障的高可用服务。HA的自动故障转移依赖于ZooKeeper的以下功能：

**1）故障检测：**集群中的每个NameNode在ZooKeeper中维护了一个持久会话，如果机器崩溃，ZooKeeper中的会话将终止，ZooKeeper通知另一个NameNode需要触发故障转移。

**2）现役NameNode选择：**ZooKeeper提供了一个简单的机制用于唯一的选择一个节点为active状态。如果目前现役NameNode崩溃，另一个节点可能从ZooKeeper获得特殊的排外锁以表明它应该成为现役NameNode。

ZKFC是自动故障转移中的另一个新组件，是ZooKeeper的客户端，也监视和管理NameNode的状态。每个运行NameNode的主机也运行了一个ZKFC进程，ZKFC负责：

**1）健康监测：**ZKFC使用一个健康检查命令定期地ping与之在相同主机的NameNode，只要该NameNode及时地回复健康状态，ZKFC认为该节点是健康的。如果该节点崩溃，冻结或进入不健康状态，健康监测器标识该节点为非健康的。

**2）ZooKeeper会话管理：**当本地NameNode是健康的，ZKFC保持一个在ZooKeeper中打开的会话。如果本地NameNode处于active状态，ZKFC也保持一个特殊的znode锁，该锁使用了ZooKeeper对短暂节点的支持，如果会话终止，锁节点将自动删除。

**3）基于ZooKeeper的选择：**如果本地NameNode是健康的，且ZKFC发现没有其它的节点当前持有znode锁，它将为自己获取该锁。如果成功，则它已经赢得了选择，并负责运行故障转移进程以使它的本地NameNode为active。故障转移进程与前面描述的手动故障转移相似，首先如果必要保护之前的现役NameNode，然后本地NameNode转换为active状态。

![在这里插入图片描述](https://img-blog.csdnimg.cn/109ad339bf3d45259b711f275b4373de.png)







## 3.Yarn

![在这里插入图片描述](https://img-blog.csdnimg.cn/d7487beda14f4be4af372b60a0001606.png)

### 3.1.工作机制详解

​    （0）Mr程序提交到客户端所在的节点

​    （1）yarnrunner向Resourcemanager申请一个application。

​    （2）rm将该应用程序的资源路径返回给yarnrunner

​    （3）该程序将运行所需资源提交到HDFS上

​    （4）程序资源提交完毕后，申请运行mrAppMaster

​    （5）RM将用户的请求初始化成一个task

​    （6）其中一个NodeManager领取到task任务。

​    （7）该NodeManager创建容器Container，并产生MRAppmaster

​    （8）Container从HDFS上拷贝资源到本地

​    （9）MRAppmaster向RM 申请运行maptask容器

​    （10）RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。

​    （11）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。

​    （12）MRAppmaster向RM申请2个容器，运行reduce task。

​    （13）reduce task向maptask获取相应分区的数据。

​    （14）程序运行完毕后，MR会向RM注销自己。